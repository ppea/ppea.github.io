<!doctype html><html lang=zh class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Welcome to int32 top. This site serves as a personal knowledge base for me to record my thoughts and ideas.  It is also a place for me to share my knowledge and experience with the world.  I hope you find something useful here."><meta name=author content=int32top><link href=https://localhost:8000/zh/09.AI/NOTE-D2L/CH3-LNN/ch3-lnn/ rel=canonical><link href=../../CH2-PRE/ch2-pre/ rel=prev><link href=../../CH4-MLP/ch4-mlp/ rel=next><link rel=icon href=../../../../../static/images/logo.png><meta name=generator content="mkdocs-1.6.0, mkdocs-material-9.5.25"><title>CH03 - 线性神经网络 - 整数之上</title><link rel=stylesheet href=../../../../../assets/stylesheets/main.6543a935.min.css><link rel=stylesheet href=../../../../../assets/stylesheets/palette.06af60db.min.css><style>.md-tag.md-tag--default-tag{--md-tag-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 80v149.5c0 17 6.7 33.3 18.7 45.3l176 176c25 25 65.5 25 90.5 0l133.5-133.5c25-25 25-65.5 0-90.5l-176-176c-12-12-28.3-18.7-45.3-18.7H48C21.5 32 0 53.5 0 80zm112 32a32 32 0 1 1 0 64 32 32 0 1 1 0-64z"/></svg>');}.md-tag.md-tag--hardware-tag{--md-tag-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M176 24c0-13.3-10.7-24-24-24s-24 10.7-24 24v40c-35.3 0-64 28.7-64 64H24c-13.3 0-24 10.7-24 24s10.7 24 24 24h40v56H24c-13.3 0-24 10.7-24 24s10.7 24 24 24h40v56H24c-13.3 0-24 10.7-24 24s10.7 24 24 24h40c0 35.3 28.7 64 64 64v40c0 13.3 10.7 24 24 24s24-10.7 24-24v-40h56v40c0 13.3 10.7 24 24 24s24-10.7 24-24v-40h56v40c0 13.3 10.7 24 24 24s24-10.7 24-24v-40c35.3 0 64-28.7 64-64h40c13.3 0 24-10.7 24-24s-10.7-24-24-24h-40v-56h40c13.3 0 24-10.7 24-24s-10.7-24-24-24h-40v-56h40c13.3 0 24-10.7 24-24s-10.7-24-24-24h-40c0-35.3-28.7-64-64-64V24c0-13.3-10.7-24-24-24s-24 10.7-24 24v40h-56V24c0-13.3-10.7-24-24-24s-24 10.7-24 24v40h-56V24zm-16 104h192c17.7 0 32 14.3 32 32v192c0 17.7-14.3 32-32 32H160c-17.7 0-32-14.3-32-32V160c0-17.7 14.3-32 32-32zm192 32H160v192h192V160z"/></svg>');}.md-tag.md-tag--software-tag{--md-tag-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 96c0-35.3 28.7-64 64-64h384c35.3 0 64 28.7 64 64v256h-64V96H128v256H64V96zM0 403.2C0 392.6 8.6 384 19.2 384h601.6c10.6 0 19.2 8.6 19.2 19.2 0 42.4-34.4 76.8-76.8 76.8H76.8C34.4 480 0 445.6 0 403.2zM281 209l-31 31 31 31c9.4 9.4 9.4 24.6 0 33.9s-24.6 9.4-33.9 0l-48-48c-9.4-9.4-9.4-24.6 0-33.9l48-48c9.4-9.4 24.6-9.4 33.9 0s9.4 24.6 0 33.9zm112-34 48 48c9.4 9.4 9.4 24.6 0 33.9l-48 48c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l31-31-31-31c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0z"/></svg>');}</style><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto+Slab:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto Slab";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config",""),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../../../../../assets/stylesheets/custom.00c04c01.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=light-blue data-md-color-accent=deep-purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#ch03- class=md-skip> 跳转至 </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label=不再显示此消息> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> For updates follow <strong>@Feng Yan</strong> on <a rel=me href=https://github.com/ppea> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </span> <strong>Github</strong> </a> </div> <script>var content,el=document.querySelector("[data-md-component=announce]");el&&(content=el.querySelector(".md-typeset"),__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0))</script> </aside> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=页眉> <a href=../../../../ title=整数之上 class="md-header__button md-logo" aria-label=整数之上 data-md-component=logo> <img src=../../../../../static/images/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> 整数之上 </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> CH03 - 线性神经网络 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=light-blue data-md-color-accent=deep-purple aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=cyan data-md-color-accent=deep-purple aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <div class=md-header__option> <div class=md-select> <button class="md-header__button md-icon" aria-label=选择当前语言> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.52 17.52 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04M18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12m-2.62 7 1.62-4.33L19.12 17h-3.24Z"/></svg> </button> <div class=md-select__inner> <ul class=md-select__list> <li class=md-select__item> <a href=../../../../../09.AI/NOTE-D2L/CH3-LNN/ch3-lnn/ hreflang=en class=md-select__link> English </a> </li> <li class=md-select__item> <a href=./ hreflang=zh class=md-select__link> 简体中文 </a> </li> </ul> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=搜索 placeholder=搜索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 320 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg> </label> <nav class=md-search__options aria-label=查找> <a href=javascript:void(0) class="md-search__icon md-icon" title=分享 aria-label=分享 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=清空当前内容 aria-label=清空当前内容 tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/ppea/ppea.github.io title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> ppea.github.io </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=导航栏 data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../../ title=整数之上 class="md-nav__button md-logo" aria-label=整数之上 data-md-component=logo> <img src=../../../../../static/images/logo.png alt=logo> </a> 整数之上 </label> <div class=md-nav__source> <a href=https://github.com/ppea/ppea.github.io title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> ppea.github.io </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../ class=md-nav__link> <span class=md-ellipsis> Index </span> </a> </li> <li class=md-nav__item> <a href=../../../../about/ class=md-nav__link> <span class=md-ellipsis> About </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../01.MATH/math/ class=md-nav__link> <span class=md-ellipsis> 01.数学 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../02.CS/CS/ class=md-nav__link> <span class=md-ellipsis> 02.计算机科学 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../03.CODING/coding/ class=md-nav__link> <span class=md-ellipsis> 03.编程 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../04.EMBEDDED/embedded-sys/ class=md-nav__link> <span class=md-ellipsis> 04.嵌入式系统 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../05.DSP/dsp/ class=md-nav__link> <span class=md-ellipsis> 05.数字信号处理 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../06.IOT/iot/ class=md-nav__link> <span class=md-ellipsis> 06.物联网 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../07.MN/mn/ class=md-nav__link> <span class=md-ellipsis> 07.移动通信 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../08.CLOUD/cloud/ class=md-nav__link> <span class=md-ellipsis> 08.云 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_11 checked> <label class=md-nav__link for=__nav_11 id=__nav_11_label tabindex=0> <span class=md-ellipsis> 09.人工智能 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_11_label aria-expanded=true> <label class=md-nav__title for=__nav_11> <span class="md-nav__icon md-icon"></span> 09.人工智能 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../ai/ class=md-nav__link> <span class=md-ellipsis> 人工智能 </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../BASICS/basics/ class=md-nav__link> <span class=md-ellipsis> 基础 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../CNN/cnn/ class=md-nav__link> <span class=md-ellipsis> 卷积神经网络 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../CV/cv/ class=md-nav__link> <span class=md-ellipsis> 计算机视觉 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../FL/fl/ class=md-nav__link> <span class=md-ellipsis> 联邦学习 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../FRAMEWORKS/frameworks/ class=md-nav__link> <span class=md-ellipsis> 框架 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../GAN/gan/ class=md-nav__link> <span class=md-ellipsis> 生成对抗网络 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../GAUSSIAN-PROCESS/gaussian-process/ class=md-nav__link> <span class=md-ellipsis> GAUSSIAN PROCESS </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../GNN/gnn/ class=md-nav__link> <span class=md-ellipsis> 图神经网络 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../KERAS/keras/ class=md-nav__link> <span class=md-ellipsis> KERAS </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../LNN/lnn/ class=md-nav__link> <span class=md-ellipsis> 线性神经网络 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../MLP/mlp/ class=md-nav__link> <span class=md-ellipsis> 多层感知机 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../MULTI-AGENT/multi-agent/ class=md-nav__link> <span class=md-ellipsis> MULTI AGENT </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../NLP/nlp/ class=md-nav__link> <span class=md-ellipsis> 自然语言处理 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_11_15 checked> <label class=md-nav__link for=__nav_11_15 id=__nav_11_15_label tabindex=0> <span class=md-ellipsis> NOTE D2L </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_11_15_label aria-expanded=true> <label class=md-nav__title for=__nav_11_15> <span class="md-nav__icon md-icon"></span> NOTE D2L </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../CH1-INTRO/ch1-intro/ class=md-nav__link> <span class=md-ellipsis> CH1 INTRO </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../CH2-PRE/ch2-pre/ class=md-nav__link> <span class=md-ellipsis> CH2 PRE </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_11_15_3 checked> <label class=md-nav__link for=__nav_11_15_3 id=__nav_11_15_3_label tabindex=0> <span class=md-ellipsis> CH3 LNN </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_11_15_3_label aria-expanded=true> <label class=md-nav__title for=__nav_11_15_3> <span class="md-nav__icon md-icon"></span> CH3 LNN </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> CH03 - 线性神经网络 </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> CH03 - 线性神经网络 </span> </a> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#31 class=md-nav__link> <span class=md-ellipsis> 3.1. 线性回归 </span> </a> <nav class=md-nav aria-label="3.1. 线性回归"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#311 class=md-nav__link> <span class=md-ellipsis> 3.1.1. 线性回归的基本元素 </span> </a> <nav class=md-nav aria-label="3.1.1. 线性回归的基本元素"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#3111 class=md-nav__link> <span class=md-ellipsis> 3.1.1.1. 线性模型 </span> </a> </li> <li class=md-nav__item> <a href=#3112 class=md-nav__link> <span class=md-ellipsis> 3.1.1.2 损失函数 </span> </a> </li> <li class=md-nav__item> <a href=#3113 class=md-nav__link> <span class=md-ellipsis> 3.1.1.3 解析解 </span> </a> </li> <li class=md-nav__item> <a href=#3114 class=md-nav__link> <span class=md-ellipsis> 3.1.1.4 随机梯度下降 </span> </a> </li> <li class=md-nav__item> <a href=#3115 class=md-nav__link> <span class=md-ellipsis> 3.1.1.5. 用模型进行预测 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#312 class=md-nav__link> <span class=md-ellipsis> 3.1.2. 矢量化加速 </span> </a> </li> <li class=md-nav__item> <a href=#313 class=md-nav__link> <span class=md-ellipsis> 3.1.3. 正态分布与平方损失 </span> </a> </li> <li class=md-nav__item> <a href=#314 class=md-nav__link> <span class=md-ellipsis> 3.1.4. 从线性回归到深度网络 </span> </a> <nav class=md-nav aria-label="3.1.4. 从线性回归到深度网络"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#3141 class=md-nav__link> <span class=md-ellipsis> 3.1.4.1. 神经网络图 </span> </a> </li> <li class=md-nav__item> <a href=#3142 class=md-nav__link> <span class=md-ellipsis> 3.1.4.2. 生物学 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#315 class=md-nav__link> <span class=md-ellipsis> 3.1.5. 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#32 class=md-nav__link> <span class=md-ellipsis> 3.2. 线性回归的从零开始实现 </span> </a> <nav class=md-nav aria-label="3.2. 线性回归的从零开始实现"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#321 class=md-nav__link> <span class=md-ellipsis> 3.2.1. 生成数据集 </span> </a> </li> <li class=md-nav__item> <a href=#322 class=md-nav__link> <span class=md-ellipsis> 3.2.2. 读取数据集 </span> </a> </li> <li class=md-nav__item> <a href=#323 class=md-nav__link> <span class=md-ellipsis> 3.2.3. 初始化模型参数 </span> </a> </li> <li class=md-nav__item> <a href=#324 class=md-nav__link> <span class=md-ellipsis> 3.2.4. 定义模型 </span> </a> </li> <li class=md-nav__item> <a href=#325 class=md-nav__link> <span class=md-ellipsis> 3.2.5. 定义损失函数 </span> </a> </li> <li class=md-nav__item> <a href=#326 class=md-nav__link> <span class=md-ellipsis> 3.2.6. 定义优化算法 </span> </a> </li> <li class=md-nav__item> <a href=#327 class=md-nav__link> <span class=md-ellipsis> 3.2.7. 训练 </span> </a> </li> <li class=md-nav__item> <a href=#328 class=md-nav__link> <span class=md-ellipsis> 3.2.8. 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#33 class=md-nav__link> <span class=md-ellipsis> 3.3. 线性回归的简洁实现 </span> </a> <nav class=md-nav aria-label="3.3. 线性回归的简洁实现"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#331 class=md-nav__link> <span class=md-ellipsis> 3.3.1. 生成数据集 </span> </a> </li> <li class=md-nav__item> <a href=#332 class=md-nav__link> <span class=md-ellipsis> 3.3.2. 读取数据集 </span> </a> </li> <li class=md-nav__item> <a href=#333 class=md-nav__link> <span class=md-ellipsis> 3.3.3. 定义模型 </span> </a> </li> <li class=md-nav__item> <a href=#334 class=md-nav__link> <span class=md-ellipsis> 3.3.4. 初始化模型参数 </span> </a> </li> <li class=md-nav__item> <a href=#335 class=md-nav__link> <span class=md-ellipsis> 3.3.5. 定义损失函数 </span> </a> </li> <li class=md-nav__item> <a href=#336 class=md-nav__link> <span class=md-ellipsis> 3.3.6. 定义优化算法 </span> </a> </li> <li class=md-nav__item> <a href=#337 class=md-nav__link> <span class=md-ellipsis> 3.3.7. 训练 </span> </a> </li> <li class=md-nav__item> <a href=#338 class=md-nav__link> <span class=md-ellipsis> 3.3.8. 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#34-softmax class=md-nav__link> <span class=md-ellipsis> 3.4. softmax回归 </span> </a> <nav class=md-nav aria-label="3.4. softmax回归"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#341 class=md-nav__link> <span class=md-ellipsis> 3.4.1. 分类问题 </span> </a> </li> <li class=md-nav__item> <a href=#342 class=md-nav__link> <span class=md-ellipsis> 3.4.2. 网络架构 </span> </a> </li> <li class=md-nav__item> <a href=#343 class=md-nav__link> <span class=md-ellipsis> 3.4.3. 全连接层的参数开销 </span> </a> </li> <li class=md-nav__item> <a href=#344-softmax class=md-nav__link> <span class=md-ellipsis> 3.4.4. softmax运算 </span> </a> </li> <li class=md-nav__item> <a href=#345 class=md-nav__link> <span class=md-ellipsis> 3.4.5. 小批量样本的矢量化 </span> </a> </li> <li class=md-nav__item> <a href=#346 class=md-nav__link> <span class=md-ellipsis> 3.4.6. 损失函数 </span> </a> <nav class=md-nav aria-label="3.4.6. 损失函数"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#3461 class=md-nav__link> <span class=md-ellipsis> 3.4.6.1 对数似然 </span> </a> </li> <li class=md-nav__item> <a href=#3462-softmax class=md-nav__link> <span class=md-ellipsis> 3.4.6.2 softmax及其导数 </span> </a> </li> <li class=md-nav__item> <a href=#3463 class=md-nav__link> <span class=md-ellipsis> 3.4.6.3 交叉熵损失 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#347 class=md-nav__link> <span class=md-ellipsis> 3.4.7. 信息论基础 </span> </a> <nav class=md-nav aria-label="3.4.7. 信息论基础"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#3471 class=md-nav__link> <span class=md-ellipsis> 3.4.7.1 熵 </span> </a> </li> <li class=md-nav__item> <a href=#3472 class=md-nav__link> <span class=md-ellipsis> 3.4.7.2 信息量 </span> </a> </li> <li class=md-nav__item> <a href=#3473 class=md-nav__link> <span class=md-ellipsis> 3.4.7.3 重新审视交叉熵 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#348 class=md-nav__link> <span class=md-ellipsis> 3.4.8. 模型预测和评估 </span> </a> </li> <li class=md-nav__item> <a href=#349 class=md-nav__link> <span class=md-ellipsis> 3.4.9. 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#35 class=md-nav__link> <span class=md-ellipsis> 3.5. 图像分类数据集 </span> </a> <nav class=md-nav aria-label="3.5. 图像分类数据集"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#351 class=md-nav__link> <span class=md-ellipsis> 3.5.1. 读取数据集 </span> </a> </li> <li class=md-nav__item> <a href=#352 class=md-nav__link> <span class=md-ellipsis> 3.5.2. 读取小批量 </span> </a> </li> <li class=md-nav__item> <a href=#353 class=md-nav__link> <span class=md-ellipsis> 3.5.3. 整合所有组件 </span> </a> </li> <li class=md-nav__item> <a href=#354 class=md-nav__link> <span class=md-ellipsis> 3.5.4. 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#36-softmax class=md-nav__link> <span class=md-ellipsis> 3.6. softmax回归的从零开始实现 </span> </a> <nav class=md-nav aria-label="3.6. softmax回归的从零开始实现"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#361 class=md-nav__link> <span class=md-ellipsis> 3.6.1. 初始化模型参数 </span> </a> </li> <li class=md-nav__item> <a href=#362-softmax class=md-nav__link> <span class=md-ellipsis> 3.6.2. 定义softmax操作 </span> </a> </li> <li class=md-nav__item> <a href=#363 class=md-nav__link> <span class=md-ellipsis> 3.6.3. 定义模型 </span> </a> </li> <li class=md-nav__item> <a href=#364 class=md-nav__link> <span class=md-ellipsis> 3.6.4. 定义损失函数 </span> </a> </li> <li class=md-nav__item> <a href=#365 class=md-nav__link> <span class=md-ellipsis> 3.6.5. 分类精度 </span> </a> </li> <li class=md-nav__item> <a href=#366 class=md-nav__link> <span class=md-ellipsis> 3.6.6. 训练 </span> </a> </li> <li class=md-nav__item> <a href=#367 class=md-nav__link> <span class=md-ellipsis> 3.6.7. 预测 </span> </a> </li> <li class=md-nav__item> <a href=#368 class=md-nav__link> <span class=md-ellipsis> 3.6.8. 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#37-softmax class=md-nav__link> <span class=md-ellipsis> 3.7. softmax回归的简洁实现 </span> </a> <nav class=md-nav aria-label="3.7. softmax回归的简洁实现"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#371 class=md-nav__link> <span class=md-ellipsis> 3.7.1. 初始化模型参数 </span> </a> </li> <li class=md-nav__item> <a href=#372-softmax class=md-nav__link> <span class=md-ellipsis> 3.7.2. 重新审视Softmax的实现 </span> </a> </li> <li class=md-nav__item> <a href=#373 class=md-nav__link> <span class=md-ellipsis> 3.7.3. 优化算法 </span> </a> </li> <li class=md-nav__item> <a href=#374 class=md-nav__link> <span class=md-ellipsis> 3.7.4. 训练 </span> </a> </li> <li class=md-nav__item> <a href=#375 class=md-nav__link> <span class=md-ellipsis> 3.7.5. 小结 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../CH4-MLP/ch4-mlp/ class=md-nav__link> <span class=md-ellipsis> CH4 MLP </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../CH5-DL-COMPUTING/ch5-dl-computing/ class=md-nav__link> <span class=md-ellipsis> CH5 DL COMPUTING </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../CH6-CNN/ch6-cnn/ class=md-nav__link> <span class=md-ellipsis> CH6 CNN </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../CH7-MODERN-CNN/ch7-modern-cnn/ class=md-nav__link> <span class=md-ellipsis> CH7 MODERN CNN </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../RL/rl/ class=md-nav__link> <span class=md-ellipsis> 强化学习 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../RNN/rnn/ class=md-nav__link> <span class=md-ellipsis> 循环神经网络 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../TL/tl/ class=md-nav__link> <span class=md-ellipsis> 迁移学习 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../TRANSFORMER/transformer/ class=md-nav__link> <span class=md-ellipsis> “变形金刚” </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../10.ROBOT/robot/ class=md-nav__link> <span class=md-ellipsis> 10.机器人 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../97.FAVORITES/favorites/ class=md-nav__link> <span class=md-ellipsis> 97.收藏夹 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../98.DEV/dev/ class=md-nav__link> <span class=md-ellipsis> 98.开发 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../99.PROJECT/project/ class=md-nav__link> <span class=md-ellipsis> 99.项目 </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#31 class=md-nav__link> <span class=md-ellipsis> 3.1. 线性回归 </span> </a> <nav class=md-nav aria-label="3.1. 线性回归"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#311 class=md-nav__link> <span class=md-ellipsis> 3.1.1. 线性回归的基本元素 </span> </a> <nav class=md-nav aria-label="3.1.1. 线性回归的基本元素"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#3111 class=md-nav__link> <span class=md-ellipsis> 3.1.1.1. 线性模型 </span> </a> </li> <li class=md-nav__item> <a href=#3112 class=md-nav__link> <span class=md-ellipsis> 3.1.1.2 损失函数 </span> </a> </li> <li class=md-nav__item> <a href=#3113 class=md-nav__link> <span class=md-ellipsis> 3.1.1.3 解析解 </span> </a> </li> <li class=md-nav__item> <a href=#3114 class=md-nav__link> <span class=md-ellipsis> 3.1.1.4 随机梯度下降 </span> </a> </li> <li class=md-nav__item> <a href=#3115 class=md-nav__link> <span class=md-ellipsis> 3.1.1.5. 用模型进行预测 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#312 class=md-nav__link> <span class=md-ellipsis> 3.1.2. 矢量化加速 </span> </a> </li> <li class=md-nav__item> <a href=#313 class=md-nav__link> <span class=md-ellipsis> 3.1.3. 正态分布与平方损失 </span> </a> </li> <li class=md-nav__item> <a href=#314 class=md-nav__link> <span class=md-ellipsis> 3.1.4. 从线性回归到深度网络 </span> </a> <nav class=md-nav aria-label="3.1.4. 从线性回归到深度网络"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#3141 class=md-nav__link> <span class=md-ellipsis> 3.1.4.1. 神经网络图 </span> </a> </li> <li class=md-nav__item> <a href=#3142 class=md-nav__link> <span class=md-ellipsis> 3.1.4.2. 生物学 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#315 class=md-nav__link> <span class=md-ellipsis> 3.1.5. 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#32 class=md-nav__link> <span class=md-ellipsis> 3.2. 线性回归的从零开始实现 </span> </a> <nav class=md-nav aria-label="3.2. 线性回归的从零开始实现"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#321 class=md-nav__link> <span class=md-ellipsis> 3.2.1. 生成数据集 </span> </a> </li> <li class=md-nav__item> <a href=#322 class=md-nav__link> <span class=md-ellipsis> 3.2.2. 读取数据集 </span> </a> </li> <li class=md-nav__item> <a href=#323 class=md-nav__link> <span class=md-ellipsis> 3.2.3. 初始化模型参数 </span> </a> </li> <li class=md-nav__item> <a href=#324 class=md-nav__link> <span class=md-ellipsis> 3.2.4. 定义模型 </span> </a> </li> <li class=md-nav__item> <a href=#325 class=md-nav__link> <span class=md-ellipsis> 3.2.5. 定义损失函数 </span> </a> </li> <li class=md-nav__item> <a href=#326 class=md-nav__link> <span class=md-ellipsis> 3.2.6. 定义优化算法 </span> </a> </li> <li class=md-nav__item> <a href=#327 class=md-nav__link> <span class=md-ellipsis> 3.2.7. 训练 </span> </a> </li> <li class=md-nav__item> <a href=#328 class=md-nav__link> <span class=md-ellipsis> 3.2.8. 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#33 class=md-nav__link> <span class=md-ellipsis> 3.3. 线性回归的简洁实现 </span> </a> <nav class=md-nav aria-label="3.3. 线性回归的简洁实现"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#331 class=md-nav__link> <span class=md-ellipsis> 3.3.1. 生成数据集 </span> </a> </li> <li class=md-nav__item> <a href=#332 class=md-nav__link> <span class=md-ellipsis> 3.3.2. 读取数据集 </span> </a> </li> <li class=md-nav__item> <a href=#333 class=md-nav__link> <span class=md-ellipsis> 3.3.3. 定义模型 </span> </a> </li> <li class=md-nav__item> <a href=#334 class=md-nav__link> <span class=md-ellipsis> 3.3.4. 初始化模型参数 </span> </a> </li> <li class=md-nav__item> <a href=#335 class=md-nav__link> <span class=md-ellipsis> 3.3.5. 定义损失函数 </span> </a> </li> <li class=md-nav__item> <a href=#336 class=md-nav__link> <span class=md-ellipsis> 3.3.6. 定义优化算法 </span> </a> </li> <li class=md-nav__item> <a href=#337 class=md-nav__link> <span class=md-ellipsis> 3.3.7. 训练 </span> </a> </li> <li class=md-nav__item> <a href=#338 class=md-nav__link> <span class=md-ellipsis> 3.3.8. 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#34-softmax class=md-nav__link> <span class=md-ellipsis> 3.4. softmax回归 </span> </a> <nav class=md-nav aria-label="3.4. softmax回归"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#341 class=md-nav__link> <span class=md-ellipsis> 3.4.1. 分类问题 </span> </a> </li> <li class=md-nav__item> <a href=#342 class=md-nav__link> <span class=md-ellipsis> 3.4.2. 网络架构 </span> </a> </li> <li class=md-nav__item> <a href=#343 class=md-nav__link> <span class=md-ellipsis> 3.4.3. 全连接层的参数开销 </span> </a> </li> <li class=md-nav__item> <a href=#344-softmax class=md-nav__link> <span class=md-ellipsis> 3.4.4. softmax运算 </span> </a> </li> <li class=md-nav__item> <a href=#345 class=md-nav__link> <span class=md-ellipsis> 3.4.5. 小批量样本的矢量化 </span> </a> </li> <li class=md-nav__item> <a href=#346 class=md-nav__link> <span class=md-ellipsis> 3.4.6. 损失函数 </span> </a> <nav class=md-nav aria-label="3.4.6. 损失函数"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#3461 class=md-nav__link> <span class=md-ellipsis> 3.4.6.1 对数似然 </span> </a> </li> <li class=md-nav__item> <a href=#3462-softmax class=md-nav__link> <span class=md-ellipsis> 3.4.6.2 softmax及其导数 </span> </a> </li> <li class=md-nav__item> <a href=#3463 class=md-nav__link> <span class=md-ellipsis> 3.4.6.3 交叉熵损失 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#347 class=md-nav__link> <span class=md-ellipsis> 3.4.7. 信息论基础 </span> </a> <nav class=md-nav aria-label="3.4.7. 信息论基础"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#3471 class=md-nav__link> <span class=md-ellipsis> 3.4.7.1 熵 </span> </a> </li> <li class=md-nav__item> <a href=#3472 class=md-nav__link> <span class=md-ellipsis> 3.4.7.2 信息量 </span> </a> </li> <li class=md-nav__item> <a href=#3473 class=md-nav__link> <span class=md-ellipsis> 3.4.7.3 重新审视交叉熵 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#348 class=md-nav__link> <span class=md-ellipsis> 3.4.8. 模型预测和评估 </span> </a> </li> <li class=md-nav__item> <a href=#349 class=md-nav__link> <span class=md-ellipsis> 3.4.9. 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#35 class=md-nav__link> <span class=md-ellipsis> 3.5. 图像分类数据集 </span> </a> <nav class=md-nav aria-label="3.5. 图像分类数据集"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#351 class=md-nav__link> <span class=md-ellipsis> 3.5.1. 读取数据集 </span> </a> </li> <li class=md-nav__item> <a href=#352 class=md-nav__link> <span class=md-ellipsis> 3.5.2. 读取小批量 </span> </a> </li> <li class=md-nav__item> <a href=#353 class=md-nav__link> <span class=md-ellipsis> 3.5.3. 整合所有组件 </span> </a> </li> <li class=md-nav__item> <a href=#354 class=md-nav__link> <span class=md-ellipsis> 3.5.4. 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#36-softmax class=md-nav__link> <span class=md-ellipsis> 3.6. softmax回归的从零开始实现 </span> </a> <nav class=md-nav aria-label="3.6. softmax回归的从零开始实现"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#361 class=md-nav__link> <span class=md-ellipsis> 3.6.1. 初始化模型参数 </span> </a> </li> <li class=md-nav__item> <a href=#362-softmax class=md-nav__link> <span class=md-ellipsis> 3.6.2. 定义softmax操作 </span> </a> </li> <li class=md-nav__item> <a href=#363 class=md-nav__link> <span class=md-ellipsis> 3.6.3. 定义模型 </span> </a> </li> <li class=md-nav__item> <a href=#364 class=md-nav__link> <span class=md-ellipsis> 3.6.4. 定义损失函数 </span> </a> </li> <li class=md-nav__item> <a href=#365 class=md-nav__link> <span class=md-ellipsis> 3.6.5. 分类精度 </span> </a> </li> <li class=md-nav__item> <a href=#366 class=md-nav__link> <span class=md-ellipsis> 3.6.6. 训练 </span> </a> </li> <li class=md-nav__item> <a href=#367 class=md-nav__link> <span class=md-ellipsis> 3.6.7. 预测 </span> </a> </li> <li class=md-nav__item> <a href=#368 class=md-nav__link> <span class=md-ellipsis> 3.6.8. 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#37-softmax class=md-nav__link> <span class=md-ellipsis> 3.7. softmax回归的简洁实现 </span> </a> <nav class=md-nav aria-label="3.7. softmax回归的简洁实现"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#371 class=md-nav__link> <span class=md-ellipsis> 3.7.1. 初始化模型参数 </span> </a> </li> <li class=md-nav__item> <a href=#372-softmax class=md-nav__link> <span class=md-ellipsis> 3.7.2. 重新审视Softmax的实现 </span> </a> </li> <li class=md-nav__item> <a href=#373 class=md-nav__link> <span class=md-ellipsis> 3.7.3. 优化算法 </span> </a> </li> <li class=md-nav__item> <a href=#374 class=md-nav__link> <span class=md-ellipsis> 3.7.4. 训练 </span> </a> </li> <li class=md-nav__item> <a href=#375 class=md-nav__link> <span class=md-ellipsis> 3.7.5. 小结 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=ch03->CH03 - 线性神经网络<a class=headerlink href=#ch03- title="Permanent link">&para;</a></h1> <p>在介绍深度神经网络之前，我们需要了解神经网络训练的基础知识。 本章我们将介绍神经网络的整个训练过程， 包括：定义简单的神经网络架构、数据处理、指定损失函数和如何训练模型。 为了更容易学习，我们将从经典算法————线性神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络， 这些知识将为本书其他部分中更复杂的技术奠定基础。</p> <h2 id=31>3.1. 线性回归<a class=headerlink href=#31 title="Permanent link">&para;</a></h2> <p><strong>回归</strong>（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。</p> <p>在机器学习领域中的大多数任务通常都与**预测**（prediction）有关。 当我们想预测一个数值时，就会涉及到回归问题。 常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、 预测需求（零售销量等）。 但不是所有的预测都是回归问题。 在后面的章节中，我们将介绍分类问题。**分类**问题的目标是预测数据属于一组类别中的哪一个。</p> <h3 id=311>3.1.1. 线性回归的基本元素<a class=headerlink href=#311 title="Permanent link">&para;</a></h3> <p>线性回归（linear regression）可以追溯到19世纪初， 它在回归的各种标准工具中最简单而且最流行。 线性回归基于几个简单的假设： 首先，假设自变量<span class=arithmatex>\(x\)</span>和因变量<span class=arithmatex>\(y\)</span>之间的关系是线性的， 即<span class=arithmatex>\(y\)</span>可以表示为<span class=arithmatex>\(x\)</span>中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。</p> <p>为了解释线性回归，我们举一个实际的例子： 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为训练数据集（training data set） 或训练集（training set）。 每行数据（比如一次房屋交易相对应的数据）称为样本（sample）， 也可以称为数据点（data point）或数据样本（data instance）。 我们把试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。 预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。</p> <h4 id=3111>3.1.1.1. 线性模型<a class=headerlink href=#3111 title="Permanent link">&para;</a></h4> <p>线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和,权重决定了每个特征对我们预测值的影响. 带权重的线性回归是输入特征的一个 仿射变换（affine transformation）。 仿射变换的特点是通过加权和对特征进行线性变换（linear transformation）， 并通过偏置项来进行平移（translation）。</p> <p>给定一个数据集，我们的目标是寻找模型的权重<span class=arithmatex>\(w\)</span>和偏置<span class=arithmatex>\(b\)</span>， 使得根据模型做出的预测大体符合数据里的真实价格。 输出的预测值由输入特征通过线性模型的仿射变换决定，仿射变换由所选权重和偏置确定。</p> <p>而在机器学习领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。 当我们的输入包含<span class=arithmatex>\(d\)</span>个特征时，我们将预测结果<span class=arithmatex>\(\hat{y}\)</span>表示为：</p> <div class=arithmatex>\[\hat{y} = w_1 x_1 + w_2 x_2 + \cdots + w_d x_d + b\]</div> <p>用向量表示，可以更简洁地表达为：</p> <div class=arithmatex>\[\hat{y} = \mathbf{w}^\top \mathbf{x} + b\]</div> <p>其中，<span class=arithmatex>\(\mathbf{w}\)</span>是权重向量，<span class=arithmatex>\(\mathbf{x}\)</span>是输入向量，<span class=arithmatex>\(b\)</span>是偏置。</p> <p>在开始寻找最好的模型参数（model parameters）<span class=arithmatex>\(w\)</span>和<span class=arithmatex>\(bb\)</span>之前， 我们还需要两个东西： （1）一种模型质量的度量方式； （2）一种能够更新模型以提高模型预测质量的方法。</p> <h4 id=3112>3.1.1.2 损失函数<a class=headerlink href=#3112 title="Permanent link">&para;</a></h4> <p>在我们开始考虑如何用模型拟合（fit）数据之前，我们需要确定一个拟合程度的度量。 损失函数（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。</p> <p>当样本<span class=arithmatex>\(i\)</span>的预测值为<span class=arithmatex>\(\hat{y}^{(i)}\)</span>，真实标签为<span class=arithmatex>\(y^{(i)}\)</span>时，平方误差可以表示为：</p> <div class=arithmatex>\[\mathcal{L}(\mathbf{w},b) = \frac{1}{n} \sum_{i=1}^n \left(\hat{y}^{(i)} - y^{(i)}\right)^2\]</div> <p>常数<span class=arithmatex>\(\frac{1}{2}\)</span>使对平方项求导后的常数系数为1，这样在形式上稍微简单一些。 但这样不会带来本质的差别。</p> <p>由于平方误差函数中的二次方项， 估计值<span class=arithmatex>\(\hat{y}^{(i)}\)</span>和观测值<span class=arithmatex>\(y^{(i)}\)</span>之间较大的差异将导致更大的损失。 为了度量模型在整个数据集上的质量，我们需计算在训练集个样本上的损失均值（也等价于求和）。</p> <div class=arithmatex>\[ \mathcal{L}(\mathbf{w},b) = \frac{1}{n} \sum_{i=1}^n \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2 \]</div> <p>在训练时，我们希望寻找一组模型参数，记为<span class=arithmatex>\(\mathbf{w}^*,b^*\)</span>，使得损失函数最小：</p> <div class=arithmatex>\[\mathcal{L}(\mathbf{w}^*,b^*) = \min_{\mathbf{w},b} \mathcal{L}(\mathbf{w},b)\]</div> <h4 id=3113>3.1.1.3 解析解<a class=headerlink href=#3113 title="Permanent link">&para;</a></h4> <p>线性回归刚好是一个很简单的优化问题。 与我们将在本书中所讲到的其他大部分模型不同，线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）。 首先，我们将偏置<span class=arithmatex>\(b\)</span>合并到参数<span class=arithmatex>\(\mathbf{w}\)</span>中。 合并方法是在所有样本上添加一列，该列恒等于1。 因此，我们只需考虑一个权重向量<span class=arithmatex>\(\mathbf{w}\)</span>，且数据样本为<span class=arithmatex>\(\mathbf{x} \in \mathbb{R}^{d+1}\)</span>。 然后，我们可以通过对模型参数<span class=arithmatex>\(\mathbf{w}\)</span>求导并令导数等于0来找到最小化损失函数的参数。 具体来说，我们需要求解以下线性方程组：</p> <div class=arithmatex>\[\mathbf{X}^\top \mathbf{X} \mathbf{w} = \mathbf{X}^\top \mathbf{y}\]</div> <p>得到的解析解为：</p> <div class=arithmatex>\[\mathbf{w} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}\]</div> <p>像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。 解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。</p> <h4 id=3114>3.1.1.4 随机梯度下降<a class=headerlink href=#3114 title="Permanent link">&para;</a></h4> <p>即使在我们无法得到解析解的情况下，我们仍然可以有效地训练模型。 在许多任务上，那些难以优化的模型效果要更好。 因此，弄清楚如何训练这些难以优化的模型是非常重要的。</p> <p>本书中我们用到一种名为**梯度下降**（gradient descent）的方法， 这种方法几乎可以优化所有深度学习模型。 <strong>它通过不断地在损失函数递减的方向上更新参数来降低误差。</strong></p> <p>梯度下降最简单的用法是**计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。** 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 <strong>因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）。</strong></p> <p>在每次迭代中，我们首先随机抽样一个小批量<span class=arithmatex>\(\mathcal{B}\)</span>，它包含了<span class=arithmatex>\(\mathcal{B}\)</span>个数据样本。 我们用<span class=arithmatex>\(\mathcal{L}(\mathbf{w},b;\mathcal{B})\)</span>来表示小批量<span class=arithmatex>\(\mathcal{B}\)</span>上的损失， 并且用<span class=arithmatex>\(\mathbf{w}\)</span>和<span class=arithmatex>\(b\)</span>表示参数。 在小批量随机梯度下降中，模型参数在每次迭代时的更新量为：</p> <div class=arithmatex>\[\mathbf{w} \leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla_{\mathbf{w}} \mathcal{L}^{(i)}(\mathbf{w},b)\]</div> <div class=arithmatex>\[b \leftarrow b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla_{b} \mathcal{L}^{(i)}(\mathbf{w},b)\]</div> <p>在上式中，<span class=arithmatex>\(\eta\)</span>（取正数）叫作**学习率**（learning rate）。 由于<span class=arithmatex>\(\mathcal{L}(\mathbf{w},b;\mathcal{B})\)</span>是关于单个样本的损失函数， 因此上式中的梯度也是基于单个样本损失函数的梯度的累加。 给定学习率<span class=arithmatex>\(\eta\)</span>和小批量随机梯度下降所使用的批量大小， 模型参数将如何更新取决于损失曲线的形状。</p> <p>我们用下面的数学公式来表示这一更新过程（<span class=arithmatex>\(\partial\)</span>符号表示偏导数）：</p> <div class=arithmatex>\[\left( \mathbf{w}, b \right) \leftarrow \left( \mathbf{w}, b \right) - \eta \left( \frac{\partial \mathcal{L}}{\partial \mathbf{w}}, \frac{\partial \mathcal{L}}{\partial b} \right)\]</div> <p>总结一下，算法的步骤如下： （1）初始化模型参数的值，如随机初始化； （2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。</p> <p><span class=arithmatex>\({|\mathcal{B}|}\)</span> 是批量大小，也是小批量随机梯度下降每次迭代的样本数， 通常取几十或几百。学习率<span class=arithmatex>\(\eta\)</span>通常取0.01、0.1、1或者10。需要注意的是，<strong>当批量大小为1时，小批量随机梯度下降退化为随机梯度下降。</strong></p> <p>批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。 这些可以调整但不在训练过程中更新的参数称为超参数（hyperparameter）。 调参（hyperparameter tuning）是选择超参数的过程。 超参数通常是我们根据训练迭代结果来调整的， 而训练迭代结果是在独立的验证数据集（validation dataset）上评估得到的。但是，即使我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值。 因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值。</p> <p>线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为**泛化（generalization）。**</p> <h4 id=3115>3.1.1.5. 用模型进行预测<a class=headerlink href=#3115 title="Permanent link">&para;</a></h4> <p>给定特征估计目标的过程通常称为**预测**（prediction）或**推断**（inference）。</p> <h3 id=312>3.1.2. 矢量化加速<a class=headerlink href=#312 title="Permanent link">&para;</a></h3> <p>在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。 为了实现这一点，需要我们对计算进行矢量化， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环。</p> <div class="tabbed-set tabbed-alternate" data-tabs=1:2><input checked=checked id=312-pytorch name=__tabbed_1 type=radio><input id=312-tensorflow name=__tabbed_1 type=radio><div class=tabbed-labels><label for=312-pytorch>PYTORCH</label><label for=312-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=o>%</span><span class=n>matplotlib</span> <span class=n>inline</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=kn>import</span> <span class=nn>math</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=kn>import</span> <span class=nn>time</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a><span class=kn>import</span> <span class=nn>torch</span>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a><span class=kn>from</span> <span class=nn>d2l</span> <span class=kn>import</span> <span class=n>torch</span> <span class=k>as</span> <span class=n>d2l</span>
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=o>%</span><span class=n>matplotlib</span> <span class=n>inline</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=kn>import</span> <span class=nn>math</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=kn>import</span> <span class=nn>time</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a><span class=kn>from</span> <span class=nn>d2l</span> <span class=kn>import</span> <span class=n>tensorflow</span> <span class=k>as</span> <span class=n>d2l</span>
</span></code></pre></div> </div> </div> </div> <h3 id=313>3.1.3. 正态分布与平方损失<a class=headerlink href=#313 title="Permanent link">&para;</a></h3> <p>正态分布和线性回归之间的关系很密切。 正态分布（normal distribution），也称为高斯分布（Gaussian distribution）， 最早由德国数学家高斯（Gauss）应用于天文学研究。 简单的说，若随机变量<span class=arithmatex>\(x\)</span>服从均值为<span class=arithmatex>\(\mu\)</span>、方差为<span class=arithmatex>\(\sigma^2\)</span>的正态分布，记为<span class=arithmatex>\(x \sim \mathcal{N}(\mu,\sigma^2)\)</span>，其概率密度函数为：</p> <div class=arithmatex>\[p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right)\]</div> <p>正态分布的均值为<span class=arithmatex>\(\mu\)</span>，方差为<span class=arithmatex>\(\sigma^2\)</span>。 当<span class=arithmatex>\(\mu = 0, \sigma = 1\)</span>时的正态分布称为标准正态分布（standard normal distribution）。 标准正态分布的概率密度函数简化为：</p> <div class=arithmatex>\[\phi(x) = \frac{1}{\sqrt{2 \pi}} \exp\left(-\frac{1}{2} x^2\right)\]</div> <p>正态分布在机器学习中有着广泛的应用。 例如，我们将在后面的章节中看到，正态分布假设了噪声的分布，而噪声又影响了数据的标签。 事实上，我们将看到许多有用的算法都假设了噪声服从正态分布。</p> <p>均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。噪声正态分布如下式:</p> <div class=arithmatex>\[ y = \mathbf{w}^\top \mathbf{x} + b + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0,\sigma^2)\]</div> <p>因此，我们现在可以写出通过给定的<span class=arithmatex>\(x\)</span>观测到特定<span class=arithmatex>\(y\)</span>的似然：</p> <div class=arithmatex>\[P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right)\]</div> <p>现在，根据极大似然估计法，参数<span class=arithmatex>\(\mathbf{w}\)</span>和<span class=arithmatex>\(b\)</span>的最优值是使整个数据集似然最大的值。</p> <p>根据极大似然估计法选择的估计量称为极大似然估计量。虽然使许多指数函数的乘积最大化看起来很困难， 但是我们可以在不改变目标的前提下，通过最大化似然对数来简化。由于历史原因，优化通常是说最小化而不是最大化。 我们可以改为最小化负对数似然<span class=arithmatex>\(-\log P(y \mid \mathbf{x})\)</span>。由此可以得到的数学公式是：</p> <div class=arithmatex>\[-\log P(y \mid \mathbf{x}) = \frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2 + \frac{1}{2} \log(2 \pi \sigma^2)\]</div> <p>现在我们只需要假设<span class=arithmatex>\(\sigma\)</span>是一个常数，就可以忽略掉最后一项，因为这个常数不会影响<span class=arithmatex>\(\mathbf{w}\)</span>和<span class=arithmatex>\(b\)</span>的选择。现在，我们的最大似然估计问题转化为最小化均方误差。因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。</p> <h3 id=314>3.1.4. 从线性回归到深度网络<a class=headerlink href=#314 title="Permanent link">&para;</a></h3> <p>到目前为止，我们只谈论了线性模型。 尽管神经网络涵盖了更多更为丰富的模型，我们依然可以用描述神经网络的方式来描述线性模型， 从而把线性模型看作一个神经网络。 首先，我们用“层”符号来重写这个模型。</p> <h4 id=3141>3.1.4.1. 神经网络图<a class=headerlink href=#3141 title="Permanent link">&para;</a></h4> <p>深度学习从业者喜欢绘制图表来可视化模型中正在发生的事情。对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换称为全连接层（fully-connected layer）或称为稠密层（dense layer）。 下一章将详细讨论由这些层组成的网络。</p> <h4 id=3142>3.1.4.2. 生物学<a class=headerlink href=#3142 title="Permanent link">&para;</a></h4> <h3 id=315>3.1.5. 小结<a class=headerlink href=#315 title="Permanent link">&para;</a></h3> <ul> <li> <p>机器学习模型中的关键要素是**训练数据**、<strong>模型</strong>、<strong>损失函数</strong>，以及**优化算法**。</p> </li> <li> <p>**矢量化**使数学表达上更简洁，同时运行的更快。</p> </li> <li> <p>**最小化目标函数**和**执行极大似然估计**等价。</p> </li> <li> <p>**线性回归模型**也是一个简单的神经网络。</p> </li> </ul> <h2 id=32>3.2. 线性回归的从零开始实现<a class=headerlink href=#32 title="Permanent link">&para;</a></h2> <p>在了解线性回归的关键思想之后，我们可以开始通过代码来动手实现线性回归了。 在这一节中，我们将从零开始实现整个方法， 包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保我们真正知道自己在做什么。 同时，了解更细致的工作原理将方便我们自定义模型、自定义层或自定义损失函数。 在这一节中，我们将只使用张量和自动求导。 在之后的章节中，我们会充分利用深度学习框架的优势，介绍更简洁的实现方式。</p> <h3 id=321>3.2.1. 生成数据集<a class=headerlink href=#321 title="Permanent link">&para;</a></h3> <p>为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。 我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。 我们将使用低维数据，这样可以很容易地将其可视化。 在下面的代码中，我们生成一个包含1000个样本的数据集， 每个样本包含从标准正态分布中采样的2个特征。 我们的合成数据集是一个矩阵<span class=arithmatex>\(\mathbf{X} \in \mathbb{R}^{1000 \times 2}\)</span>。</p> <p>我们使用线性模型参数<span class=arithmatex>\(\mathbf{w} = [2, -3.4]^\top\)</span>、<span class=arithmatex>\(b = 4.2\)</span>和噪声项<span class=arithmatex>\(\epsilon\)</span>生成数据集及其标签：</p> <div class=arithmatex>\[\mathbf{y} = \mathbf{X} \mathbf{w} + b + \mathbf{\epsilon}\]</div> <p><span class=arithmatex>\(\mathbf{\epsilon}\)</span>可以视为模型预测和标签时的潜在观测误差。 在这里我们认为标准假设成立，即服从均值为0的正态分布。 为了简化问题，我们将标准差设为0.01。</p> <h3 id=322>3.2.2. 读取数据集<a class=headerlink href=#322 title="Permanent link">&para;</a></h3> <p>回想一下，训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。 由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数， 该函数能打乱数据集中的样本并以小批量方式获取数据。 当我们运行迭代时，我们会连续地获得不同的小批量，直至遍历完整个数据集。 上面实现的迭代对教学来说很好，但它的执行效率很低，可能会在实际问题上陷入麻烦。 例如，它要求我们将所有数据加载到内存中，并执行大量的随机内存访问。 在深度学习框架中实现的内置迭代器效率要高得多， 它可以处理存储在文件中的数据和数据流提供的数据。</p> <h3 id=323>3.2.3. 初始化模型参数<a class=headerlink href=#323 title="Permanent link">&para;</a></h3> <p>在我们开始用小批量随机梯度下降优化我们的模型参数之前， 我们需要先有一些参数。 在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0。 在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。 有了这个梯度，我们就可以向减小损失的方向更新每个参数。 因为手动计算梯度很枯燥而且容易出错，所以没有人会手动计算梯度。 我们使用 2.5节中引入的自动微分来计算梯度。</p> <h3 id=324>3.2.4. 定义模型<a class=headerlink href=#324 title="Permanent link">&para;</a></h3> <p>接下来，我们必须定义模型，将模型的输入和参数同模型的输出关联起来。回想一下，要计算线性模型的输出， 我们只需计算输入特征<span class=arithmatex>\(X\)</span>和模型权重<span class=arithmatex>\(w\)</span>的矩阵-向量乘法后加上偏置<span class=arithmatex>\(b\)</span>。 注意，上面的<span class=arithmatex>\(Xw\)</span>是一个向量，而<span class=arithmatex>\(b\)</span>是一个标量。回顾向量相加的广播机制，我们可以直接将<span class=arithmatex>\(b\)</span>加到<span class=arithmatex>\(Xw\)</span>的每一行。</p> <h3 id=325>3.2.5. 定义损失函数<a class=headerlink href=#325 title="Permanent link">&para;</a></h3> <p>因为需要计算损失函数的梯度，所以我们应该先定义损失函数。 这里我们使用 3.1节中描述的平方损失函数。 在实现中，我们需要将真实值y的形状转换为和预测值y_hat的形状相同。</p> <h3 id=326>3.2.6. 定义优化算法<a class=headerlink href=#326 title="Permanent link">&para;</a></h3> <div class="admonition note"> <p class=admonition-title>Note</p> <p>在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。 接下来，朝着减少损失的方向更新我们的参数。</p> </div> <h3 id=327>3.2.7. 训练<a class=headerlink href=#327 title="Permanent link">&para;</a></h3> <p>现在我们已经准备好了模型训练所有需要的要素，可以实现主要的训练过程部分了。 理解这段代码至关重要，因为从事深度学习后， 相同的训练过程几乎一遍又一遍地出现。 在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。 计算完损失后，我们开始反向传播，存储每个参数的梯度。 最后，我们调用优化算法sgd来更新模型参数。</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>概括一下，我们将执行以下循环： - 初始化参数 - 重复，直到完成 - 计算梯度 - 更新参数</p> </div> <p>注意，我们不应该想当然地认为我们能够完美地求解参数。 <strong>在机器学习中，我们通常不太关心恢复真正的参数，而更关心如何高度准确预测参数。</strong> 幸运的是，即使是在复杂的优化问题上，随机梯度下降通常也能找到非常好的解。 其中一个原因是，在深度网络中存在许多参数组合能够实现高度精确的预测。</p> <h3 id=328>3.2.8. 小结<a class=headerlink href=#328 title="Permanent link">&para;</a></h3> <ul> <li>我们学习了深度网络是如何实现和优化的。在这一过程中只使用张量和自动微分，不需要定义层或复杂的优化器。</li> <li>这一节只触及到了表面知识。在下面的部分中，我们将基于刚刚介绍的概念描述其他模型，并学习如何更简洁地实现其他模型。</li> </ul> <h2 id=33>3.3. 线性回归的简洁实现<a class=headerlink href=#33 title="Permanent link">&para;</a></h2> <p>在过去的几年里，出于对深度学习强烈的兴趣， 许多公司、学者和业余爱好者开发了各种成熟的开源框架。 这些框架可以自动化基于梯度的学习算法中重复性的工作。 在 3.2节中，我们只运用了： （1）通过张量来进行数据存储和线性代数； （2）通过自动微分来计算梯度。 实际上，由于数据迭代器、损失函数、优化器和神经网络层很常用， 现代深度学习库也为我们实现了这些组件。</p> <p>本节将介绍如何通过使用深度学习框架来简洁地实现 3.2节中的线性回归模型。</p> <h3 id=331>3.3.1. 生成数据集<a class=headerlink href=#331 title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=332>3.3.2. 读取数据集<a class=headerlink href=#332 title="Permanent link">&para;</a></h3> <p>我们可以调用框架中现有的API来读取数据。 我们将features和labels作为API的参数传递，并通过数据迭代器指定batch_size。 此外，布尔值is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据。</p> <h3 id=333>3.3.3. 定义模型<a class=headerlink href=#333 title="Permanent link">&para;</a></h3> <p>当我们在 3.2节中实现线性回归时， 我们明确定义了模型参数变量，并编写了计算的代码，这样通过基本的线性代数运算得到输出。 但是，如果模型变得更加复杂，且当我们几乎每天都需要实现模型时，自然会想简化这个过程。 这种情况类似于为自己的博客从零开始编写网页。 做一两次是有益的，但如果每个新博客就需要工程师花一个月的时间重新开始编写网页，那并不高效。</p> <p>对于标准深度学习模型，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。 我们首先定义一个模型变量net，它是一个Sequential类的实例。 Sequential类将多个层串联在一起。 当给定输入数据时，Sequential实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。 在下面的例子中，我们的模型只包含一个层，因此实际上不需要Sequential。 但是由于以后几乎所有的模型都是多层的，在这里使用Sequential会让你熟悉“标准的流水线”。</p> <p>回顾单层网络架构， 这一单层被称为全连接层（fully-connected layer）， 因为它的每一个输入都通过矩阵-向量乘法得到它的每个输出。</p> <h3 id=334>3.3.4. 初始化模型参数<a class=headerlink href=#334 title="Permanent link">&para;</a></h3> <p>在使用net之前，我们需要初始化模型参数。 如在线性回归模型中的权重和偏置。 深度学习框架通常有预定义的方法来初始化参数。 在这里，我们指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样， 偏置参数将初始化为零。</p> <h3 id=335>3.3.5. 定义损失函数<a class=headerlink href=#335 title="Permanent link">&para;</a></h3> <p>计算均方误差使用的是MSELoss类，也称为平方范数。 默认情况下，它返回所有样本损失的平均值。</p> <h3 id=336>3.3.6. 定义优化算法<a class=headerlink href=#336 title="Permanent link">&para;</a></h3> <p>小批量随机梯度下降算法是一种优化神经网络的标准工具， PyTorch在optim模块中实现了该算法的许多变种。 当我们实例化一个SGD实例时，我们要指定优化的参数 （可通过net.parameters()从我们的模型中获得）以及优化算法所需的超参数字典。 小批量随机梯度下降只需要设置lr值，这里设置为0.03。</p> <h3 id=337>3.3.7. 训练<a class=headerlink href=#337 title="Permanent link">&para;</a></h3> <p>通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。 我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。 当我们需要更复杂的模型时，高级API的优势将大大增加。 当我们有了所有的基本组件，训练过程代码与我们从零开始实现时所做的非常相似。</p> <p>回顾一下：在每个迭代周期里，我们将完整遍历一次数据集（train_data）， 不停地从中获取一个小批量的输入和相应的标签。 对于每一个小批量，我们会进行以下步骤:</p> <ul> <li>通过调用net(X)生成预测并计算损失l（正向传播）。</li> <li>通过进行反向传播来计算梯度。</li> <li>通过调用优化算法迭代模型参数。</li> </ul> <p>为了更好的衡量训练效果，我们计算每个迭代周期后的损失，并打印它来监控训练过程。</p> <p>下面我们比较生成数据集的真实参数和通过有限数据训练获得的模型参数。 要访问参数，我们首先从net访问所需的层，然后读取该层的权重和偏置。 正如在从零开始实现中一样，我们估计得到的参数与生成数据的真实参数非常接近。</p> <h3 id=338>3.3.8. 小结<a class=headerlink href=#338 title="Permanent link">&para;</a></h3> <ul> <li> <p>我们可以使用PyTorch的高级API更简洁地实现模型。</p> </li> <li> <p>在PyTorch中，data模块提供了数据处理工具，nn模块定义了大量的神经网络层和常见损失函数。</p> </li> <li> <p>我们可以通过_结尾的方法将参数替换，从而初始化参数。</p> </li> </ul> <h2 id=34-softmax>3.4. softmax回归<a class=headerlink href=#34-softmax title="Permanent link">&para;</a></h2> <p>在 3.1节中我们介绍了线性回归。 随后，在 3.2节中我们从头实现线性回归。 然后，在 3.3节中我们使用深度学习框架的高级API简洁实现线性回归。</p> <p>回归可以用于预测多少的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。</p> <p>事实上，我们也对分类问题感兴趣：不是问“多少”，而是问“哪一个”：</p> <p>某个电子邮件是否属于垃圾邮件文件夹？</p> <p>某个用户可能注册或不注册订阅服务？</p> <p>某个图像描绘的是驴、狗、猫、还是鸡？</p> <p>某人接下来最有可能看哪部电影？</p> <p>通常，机器学习实践者用分类这个词来描述两个有微妙差别的问题： 1. 我们只对样本的“硬性”类别感兴趣，即属于哪个类别； 2. 我们希望得到“软性”类别，即得到属于每个类别的概率。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。</p> <h3 id=341>3.4.1. 分类问题<a class=headerlink href=#341 title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=342>3.4.2. 网络架构<a class=headerlink href=#342 title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=343>3.4.3. 全连接层的参数开销<a class=headerlink href=#343 title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=344-softmax>3.4.4. softmax运算<a class=headerlink href=#344-softmax title="Permanent link">&para;</a></h3> <p>社会科学家邓肯·卢斯于1959年在选择模型（choice model）的理论基础上 发明的softmax函数正是这样做的： softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。</p> <div class=arithmatex>\[\hat{y}_1, \ldots, \hat{y}_n = \exp(o_1), \ldots, \exp(o_n)\]</div> <div class=arithmatex>\[\hat{y}_1, \ldots, \hat{y}_n = \frac{\exp(o_1)}{\sum_{i=1}^n \exp(o_i)}, \ldots, \frac{\exp(o_n)}{\sum_{i=1}^n \exp(o_i)}\]</div> <p>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）。</p> <h3 id=345>3.4.5. 小批量样本的矢量化<a class=headerlink href=#345 title="Permanent link">&para;</a></h3> <p>先仿射变换，后softmax运算。 为了提高计算效率，我们通常会对小批量数据执行矢量计算。 假设我们的输入样本数为<span class=arithmatex>\(|\mathcal{B}|\)</span>，输入特征的维度为<span class=arithmatex>\(n\)</span>，输出个数为<span class=arithmatex>\(q。 仿射变换的输出为一个\)</span>|\mathcal{B}| \times q<span class=arithmatex>\(矩阵，其第\)</span>i<span class=arithmatex>\(行是样本\)</span>i<span class=arithmatex>\(的输出。 因此，我们定义矩阵\)</span>\mathbf{X}<span class=arithmatex>\(为一个\)</span>|\mathcal{B}| \times n<span class=arithmatex>\(的小批量样本特征。 我们得到矩阵\)</span>\mathbf{O}<span class=arithmatex>\(，它是\)</span>\mathbf{X}<span class=arithmatex>\(和\)</span>\mathbf{W}<span class=arithmatex>\(的矩阵-矩阵乘法后再加上偏置\)</span>\mathbf{b}<span class=arithmatex>\(。 其中\)</span>\mathbf{W}<span class=arithmatex>\(是\)</span>n \times q<span class=arithmatex>\(的权重矩阵，\)</span>\mathbf{b}<span class=arithmatex>\(是\)</span>q<span class=arithmatex>\(维的偏置。 对于矩阵\)</span>\mathbf{O}<span class=arithmatex>\(中的任意行\)</span>i<span class=arithmatex>\(，它对应了样本\)</span>i<span class=arithmatex>\(的输出。 因此，我们可以通过对矩阵\)</span>\mathbf{O}<span class=arithmatex>\(中的所有元素做softmax运算来得到输出。 <span class=arithmatex>\(o_{ij}\)</span>表示样本\)</span>i<span class=arithmatex>\(中输出\)</span>j<span class=arithmatex>\(的未归一化预测，\)</span>\hat{y}_{ij}<span class=arithmatex>\(表示样本\)</span>i<span class=arithmatex>\(中输出\)</span>j$的预测概率。</p> <h3 id=346>3.4.6. 损失函数<a class=headerlink href=#346 title="Permanent link">&para;</a></h3> <p>接下来，我们需要一个损失函数来度量预测的效果。 我们将使用最大似然估计，这与在线性回归 （ 3.1.3节） 中的方法相同。</p> <h4 id=3461>3.4.6.1 对数似然<a class=headerlink href=#3461 title="Permanent link">&para;</a></h4> <p>~</p> <h4 id=3462-softmax>3.4.6.2 softmax及其导数<a class=headerlink href=#3462-softmax title="Permanent link">&para;</a></h4> <p>~</p> <h4 id=3463>3.4.6.3 交叉熵损失<a class=headerlink href=#3463 title="Permanent link">&para;</a></h4> <p>~</p> <h3 id=347>3.4.7. 信息论基础<a class=headerlink href=#347 title="Permanent link">&para;</a></h3> <p>信息论（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。</p> <h4 id=3471>3.4.7.1 熵<a class=headerlink href=#3471 title="Permanent link">&para;</a></h4> <p>信息论的核心思想是量化数据中的信息内容。 在信息论中，该数值被称为分布<span class=arithmatex>\(P\)</span>的熵（entropy）。 可以通过以下方程得到：</p> <div class=arithmatex>\[H[P] = \sum_j - p(j) \log p(j)\]</div> <p>信息论的基本定理之一指出，为了对从分布<span class=arithmatex>\(p\)</span>中抽取的数据进行编码，我们至少需要<span class=arithmatex>\(H[P]\)</span>“纳特”对其进行编码。 “纳特”相当于比特(bit)，但是对数底为e而不是2.因此，一个纳特是<span class=arithmatex>\(\frac{1}{\log 2}\)</span>比特。</p> <h4 id=3472>3.4.7.2 信息量<a class=headerlink href=#3472 title="Permanent link">&para;</a></h4> <p>压缩与预测有什么关系呢？ 想象一下，我们有一个要压缩的数据流。 如果我们很容易预测下一个数据，那么这个数据就很容易压缩。 为什么呢？ 举一个极端的例子，假如数据流中的每个数据完全相同，这会是一个非常无聊的数据流。 由于它们总是相同的，我们总是知道下一个数据是什么。 所以，为了传递数据流的内容，我们不必传输任何信息。也就是说，“下一个数据是xx”这个事件毫无信息量。</p> <p>但是，如果我们不能完全预测每一个事件，那么我们有时可能会感到“惊异”。 克劳德·香农决定用信息量<span class=arithmatex>\(\log \frac{1}{p(j)} = -\log p(j)\)</span>来量化这种惊异。 在观察一个事件<span class=arithmatex>\(j\)</span>时，并赋予它（主观）概率<span class=arithmatex>\(P(j)\)</span>。 当我们赋予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。 在 (3.4.11)中定义的熵， 是当分配的概率真正匹配数据生成过程时的信息量的期望。</p> <h4 id=3473>3.4.7.3 重新审视交叉熵<a class=headerlink href=#3473 title="Permanent link">&para;</a></h4> <p>如果把熵<span class=arithmatex>\(H(p)\)</span>想象为“知道真实概率的人所经历的惊异程度”，那么什么是交叉熵？交叉熵从<span class=arithmatex>\(P\)</span>到<span class=arithmatex>\(Q\)</span>,记为<span class=arithmatex>\(H(P, Q)\)</span>,我们可以把交叉熵想象为“主观概率为<span class=arithmatex>\(Q\)</span>的观察者在看到根据概率<span class=arithmatex>\(P\)</span>产生的事件时的惊异程度”。 当<span class=arithmatex>\(P=Q\)</span>时，交叉熵达到最低。在这种情况下，从<span class=arithmatex>\(P\)</span>到<span class=arithmatex>\(Q\)</span>的交叉熵是<span class=arithmatex>\(H(P, P) = H(P)\)</span>。 </p> <p>简而言之，我们可以从两方面来考虑交叉熵分类目标： - （i）最大化观测数据的似然； - （ii）最小化传达标签所需的惊异。</p> <h3 id=348>3.4.8. 模型预测和评估<a class=headerlink href=#348 title="Permanent link">&para;</a></h3> <p>在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。 通常我们使用预测概率最高的类别作为输出类别。 如果预测与实际类别（标签）一致，则预测是正确的。 在接下来的实验中，我们将使用精度（accuracy）来评估模型的性能。 精度等于正确预测数与预测总数之间的比率。</p> <h3 id=349>3.4.9. 小结<a class=headerlink href=#349 title="Permanent link">&para;</a></h3> <ul> <li> <p>softmax运算获取一个向量并将其映射为概率。</p> </li> <li> <p>softmax回归适用于分类问题，它使用了softmax运算中输出类别的概率分布。</p> </li> <li> <p>交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数。</p> </li> </ul> <h2 id=35>3.5. 图像分类数据集<a class=headerlink href=#35 title="Permanent link">&para;</a></h2> <p>MNIST数据集 (LeCun et al., 1998) 是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的Fashion-MNIST数据集 (Xiao et al., 2017)。</p> <h3 id=351>3.5.1. 读取数据集<a class=headerlink href=#351 title="Permanent link">&para;</a></h3> <p>我们可以通过框架中的内置函数将Fashion-MNIST数据集下载并读取到内存中。</p> <h3 id=352>3.5.2. 读取小批量<a class=headerlink href=#352 title="Permanent link">&para;</a></h3> <p>为了使我们在读取训练集和测试集时更容易，我们使用内置的数据迭代器，而不是从零开始创建。 回顾一下，在每次迭代中，数据加载器每次都会读取一小批量数据，大小为batch_size。 通过内置数据迭代器，我们可以随机打乱了所有样本，从而无偏见地读取小批量。</p> <h3 id=353>3.5.3. 整合所有组件<a class=headerlink href=#353 title="Permanent link">&para;</a></h3> <p>现在我们定义load_data_fashion_mnist函数，用于获取和读取Fashion-MNIST数据集。 这个函数返回训练集和验证集的数据迭代器。 此外，这个函数还接受一个可选参数resize，用来将图像大小调整为另一种形状。</p> <h3 id=354>3.5.4. 小结<a class=headerlink href=#354 title="Permanent link">&para;</a></h3> <ul> <li>Fashion-MNIST是一个服装分类数据集，由10个类别的图像组成。我们将在后续章节中使用此数据集来评估各种分类算法。</li> <li>我们将高度h和宽度w像素的图像的形状记为h×w或（h，w）。</li> <li>数据迭代器是深度学习框架中一个重要的组件。数据迭代器提供了随机读取小批量数据样本的便利方式。</li> </ul> <h2 id=36-softmax>3.6. softmax回归的从零开始实现<a class=headerlink href=#36-softmax title="Permanent link">&para;</a></h2> <p>就像我们从零开始实现线性回归一样， 我们认为softmax回归也是重要的基础，因此应该知道实现softmax回归的细节。 本节我们将使用刚刚在 3.5节中引入的Fashion-MNIST数据集， 并设置数据迭代器的批量大小为256。</p> <h3 id=361>3.6.1. 初始化模型参数<a class=headerlink href=#361 title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=362-softmax>3.6.2. 定义softmax操作<a class=headerlink href=#362-softmax title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=363>3.6.3. 定义模型<a class=headerlink href=#363 title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=364>3.6.4. 定义损失函数<a class=headerlink href=#364 title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=365>3.6.5. 分类精度<a class=headerlink href=#365 title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=366>3.6.6. 训练<a class=headerlink href=#366 title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=367>3.6.7. 预测<a class=headerlink href=#367 title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=368>3.6.8. 小结<a class=headerlink href=#368 title="Permanent link">&para;</a></h3> <ul> <li>借助softmax回归，我们可以训练多分类的模型。</li> <li>训练softmax回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。</li> </ul> <h2 id=37-softmax>3.7. softmax回归的简洁实现<a class=headerlink href=#37-softmax title="Permanent link">&para;</a></h2> <p>在 3.3节中， 我们发现通过深度学习框架的高级API能够使实现</p> <p>线性回归变得更加容易。 同样，通过深度学习框架的高级API也能更方便地实现softmax回归模型。 本节如在 3.6节中一样， 继续使用Fashion-MNIST数据集，并保持批量大小为256。</p> <h3 id=371>3.7.1. 初始化模型参数<a class=headerlink href=#371 title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=372-softmax>3.7.2. 重新审视Softmax的实现<a class=headerlink href=#372-softmax title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=373>3.7.3. 优化算法<a class=headerlink href=#373 title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=374>3.7.4. 训练<a class=headerlink href=#374 title="Permanent link">&para;</a></h3> <p>~</p> <h3 id=375>3.7.5. 小结<a class=headerlink href=#375 title="Permanent link">&para;</a></h3> <ul> <li>使用深度学习框架的高级API，我们可以更简洁地实现softmax回归。</li> <li>从计算的角度来看，实现softmax回归比较复杂。在许多情况下，深度学习框架在这些著名的技巧之外采取了额外的预防措施，来确保数值的稳定性。这使我们避免了在实践中从零开始编写模型时可能遇到的陷阱。</li> </ul> <form class=md-feedback name=feedback hidden> <fieldset> <legend class=md-feedback__title> Was this page helpful? </legend> <div class=md-feedback__inner> <div class=md-feedback__list> <button class="md-feedback__icon md-icon" type=submit title="This page was helpful" data-md-value=1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M5 9v12H1V9h4m4 12a2 2 0 0 1-2-2V9c0-.55.22-1.05.59-1.41L14.17 1l1.06 1.06c.27.27.44.64.44 1.05l-.03.32L14.69 8H21a2 2 0 0 1 2 2v2c0 .26-.05.5-.14.73l-3.02 7.05C19.54 20.5 18.83 21 18 21H9m0-2h9.03L21 12v-2h-8.79l1.13-5.32L9 9.03V19Z"/></svg> </button> <button class="md-feedback__icon md-icon" type=submit title="This page could be improved" data-md-value=0> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 15V3h4v12h-4M15 3a2 2 0 0 1 2 2v10c0 .55-.22 1.05-.59 1.41L9.83 23l-1.06-1.06c-.27-.27-.44-.64-.44-1.06l.03-.31.95-4.57H3a2 2 0 0 1-2-2v-2c0-.26.05-.5.14-.73l3.02-7.05C4.46 3.5 5.17 3 6 3h9m0 2H5.97L3 12v2h8.78l-1.13 5.32L15 14.97V5Z"/></svg> </button> </div> <div class=md-feedback__note> <div data-md-value=1 hidden> Thanks for your feedback! </div> <div data-md-value=0 hidden> Thanks for your feedback! Help us improve this page by using our <a href=... target=_blank rel=noopener>feedback form</a>. </div> </div> </div> </fieldset> </form> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> 回到页面顶部 </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=页脚> <a href=../../CH2-PRE/ch2-pre/ class="md-footer__link md-footer__link--prev" aria-label="上一页: CH02 - 预备知识"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 320 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> 上一页 </span> <div class=md-ellipsis> CH02 - 预备知识 </div> </div> </a> <a href=../../CH4-MLP/ch4-mlp/ class="md-footer__link md-footer__link--next" aria-label="下一页: 多层感知机"> <div class=md-footer__title> <span class=md-footer__direction> 下一页 </span> <div class=md-ellipsis> 多层感知机 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 320 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2021 ~ now | int32top </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://ppea.github.io target=_blank rel=noopener title=ppea.github.io class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M575.8 255.5c0 18-15 32.1-32 32.1h-32l.7 160.2c0 2.7-.2 5.4-.5 8.1V472c0 22.1-17.9 40-40 40h-16c-1.1 0-2.2 0-3.3-.1-1.4.1-2.8.1-4.2.1H392c-22.1 0-40-17.9-40-40v-88c0-17.7-14.3-32-32-32h-64c-17.7 0-32 14.3-32 32v88c0 22.1-17.9 40-40 40h-55.9c-1.5 0-3-.1-4.5-.2-1.2.1-2.4.2-3.6.2h-16c-22.1 0-40-17.9-40-40V360c0-.9 0-1.9.1-2.8v-69.6H32c-18 0-32-14-32-32.1 0-9 3-17 10-24L266.4 8c7-7 15-8 22-8s15 2 21 7l255.4 224.5c8 7 12 15 11 24z"/></svg> </a> <a href=https://github.com/ppea target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../../../..", "features": ["announce.dismiss", "content.code.annotate", "content.tabs.link", "header.autohide", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.path", "navigation.prune", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "tags": {"Default": "default-tag", "Hardware": "hardware-tag", "Software": "software-tag"}, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script> <script src=../../../../../assets/javascripts/bundle.081f42fc.min.js></script> <script src=../../../../../assets/javascripts/custom.2340dcd7.min.js></script> </body> </html>